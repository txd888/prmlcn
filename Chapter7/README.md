在前一章中，我们研究了许多基于非线性核的学习算法。这种算法的一个最大的局限性是核 函数$$ k(x_n,x_m) $$必须对所有可能的训练点对xn和xm进行求值，这在训练阶段的计算上是不可行的，且会使得对新的数据点进行预测时也会花费过多的时间。

本章中，我们会看到具有稀疏（sparse）解的基于核的算法，从而对新数据的预测只依赖于在训练数据点的一个子集上计算的核函数。首先，我们详细讨论支持向量机（support vector machine）（SVM），它在一些年之前变得逐渐流行，可以用来解决分类问题、回归问题以及异常点检测问题。支持向量机的一个重要性质是模型参数的确定对应于一个凸最优化问题，因此许多局部解也是全局最优解。由于对支持向量机的讨论需要频繁用到拉格朗日乘数法，因此我们建议读者复习附录E中提到的关键的概念。额外的关于支持向量机的介绍，可以参考Vapnik(1995)、Burges(1998)、Cristianini and Shawe-Taylor(2000)、Müller et al.(2001)、Scholkopf and Smola(2002)和Herbrich(2002)。    

SVM是一个决策机器，因此不提供后验概率。我们已经在1.5.4节讨论过了确定概率的好处。 另一种稀疏核方法，被称为相关向量机（relevance vector machine）（RVM），基于贝叶斯方法，提供了后验概率的输出，并且通常能产生比SVM更稀疏的解。    

