蒙特卡罗方法除了为贝叶斯框架的直接实现提供了原理，还在频率学家的框架内起着重要的作用，例如寻找最大似然解。特别地，对于EM算法中的E步骤无法解析地计算的模型，采样方法也可以用来近似E步骤。考虑一个模型，它的隐含变量为$$ Z $$，可见（观测）变量为$$ X $$，参数 为$$ \theta $$。在M步骤中关于$$ \theta $$最大化的步骤为完整数据对数似然的期望，形式为    

$$
Q(\theta,\theta^{old}) = \int p(Z|X,\theta^{old})\ln p(Z,X|\theta)dZ \tag{11.28}
$$    

我们可以使用采样方法来近似这个积分，方法是计算样本$$ \{Z^{(l)}\} $$上的有限和，这些样本是从当前的对后验概率分布$$ p(Z|X, θ^{old}) $$的估计中抽取的，即     

$$
Q(\theta,\theta^{old}) \simeq \frac{1}{L}\sum\limits_{l=1}^L\ln p(Z^{(l)},X|\theta) \tag{11.29}
$$    

然后，$$ Q $$函数在M步骤中使用通常的步骤进行优化。这个步骤被称为蒙特卡罗EM算法（Monte Carlo EM algorithm）。    

将这种方法推广到寻找$$ \theta $$上的后验概率的峰值（MAP估计）的问题是很容易的，其中先验概率分布$$ p(\theta) $$已经被定义。我们只需在进行M步骤之前，在函数$$ Q(\theta, \theta^{old}) $$中加上$$ \ln p(\theta) $$即可。    

蒙特卡罗EM算法的一个特定的情形，被称为随机EM（stochastic EM）。如果我们考虑有限数量的概率分布组成的混合模型，并且在每个E步骤中只抽取一个样本时，我们就会用到这种算法。这里，潜在变量$$ Z $$描述了$$ K $$个混合分量中的哪个分量被用于生成每个数据点。在E步骤中，$$ Z $$的样本从后验概率分布$$ p(Z|X, \theta^{old}) $$中抽取，其中X是数据集。这高效地将每个数据点硬性地分配到混合分布中的一个分量中。在M步骤中，对于后验概率分布的这个采样的近似被用于按照平常的方式更新模型的参数。    

现在假设我们从最大似然的方法转移到纯粹的贝叶斯方法，其中我们希望从参数向量$$ \theta $$上的后验概率分布中进行采样。原则上，我们希望从联合后验分布$$ p(\theta, Z|X) $$中抽取样本，但是我们假设这个计算十分困难。进一步的，我们假设从完整数据参数的后验概率分布$$ p(\theta|Z,X) $$中进行采样相对简单。这就产生了数据增广算法（data augmentation
algorithm），它在两个步骤之间交替进行，这两个步骤被称为I步骤（归咎(imputation)步骤，类似于E步骤）和P步骤（后验(posterior)步骤，类似于M步骤）。    

I步骤。我们希望从概率分布$$ p(Z|X) $$采样，但是我们不能直接进行。于是，我们注意到下面的关系    

$$
p(Z|X) = \int p(Z|\theta,X)p(\theta|X)d\theta \tag{11.30}
$$    

因此对于$$ l = 1,...,L $$，我们首先从当前对$$ p(\theta|X) $$的估计中抽取样本$$ \theta^{(l)} $$，然后使用这个样本从$$ p(Z|\theta^{(l)}, X) $$中抽取样本$$ Z^{(l)} $$。    

P步骤。给定关系    

$$
p(\theta|X) = \int p(\theta|Z,X)p(Z|X)dZ \tag{11.31}
$$    

我们使用从I步骤中得到的样本$$ \{Z^{(l)}\} $$，计算$$ \theta $$上的后验概率分布的修正后的估计，结果为    

$$
p(\theta|X) \simeq \frac{1}{L}\sum\limits_{l=1}^Lp(\theta|Z^{(l)},X) \tag{11.32}
$$    

根据假设，在I步骤中从这个近似分布中采样是可行的。    

注意，我们对参数$$ \theta $$和隐含变量$$ Z $$进行了（多少有些人为的）区分。从现在开始，我们不进行这种区分，仅仅集中于从给定的后验概率分布中抽取样本的问题。    


