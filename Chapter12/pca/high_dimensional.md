在主成分分析的一些应用中，数据点的数量小于数据空间的维度。例如，我们可能希望将PCA应用于由几百张图片组成的数据集，每个图片对应于几百万维（对应于图像中每个像素的三个颜色值）空间中的一个向量。注意，在一个$$ D $$维空间中，$$ N $$个数据点$$ (N < D) $$定义了一个线性子空间，它的维度最多为$$ N − 1 $$，因此在使用PCA时，几乎没有$$ M $$大于$$ N − 1 $$的数据点。实际上，如果我们运行PCA，我们会发现至少$$ D − N + 1
$$个特征值为0，对应于沿着数据集的方差为零的方向的特征向量。此外，通常的寻找$$ D \times D $$矩阵的特征向量的算法的计算代价为$$ O(D^3) $$，因此对于诸如图像这种应用来说，直接应用PCA在计算上是不可行的。         

我们可以这样解这个问题。首先，让我们将$$ X $$定义为$$ (N \times D) $$维中心数据矩阵，它的第$$ n $$行为$$ (x_n − \bar{x})^T $$。这样，协方差矩阵（12.3）可以写成$$ S = N^{−1}X^TX $$， 对应的特征向量方程变成了    

$$
\frac{1}{N}X^TXu_i = \lambda_iu_i \tag{12.26}
$$    

现在，将两侧左乘$$ X $$，可得    

$$
\frac{1}{N}XX^T(Xu_i) = \lambda_i(Xu_i) \tag{12.27}
$$     

如果我们现在定义$$ v_i = Xu_i $$，那么我们有    

$$
\frac{1}{N}XX^Tv_i = \lambda_iv_i \tag{12.28}
$$     

它是$$ N \times N $$矩阵$$ N^{−1}XX^T $$的一个特征向量方程。我们看到这个矩阵与原始的协方差矩阵具有相同的$$ N − 1 $$个特征值，原始的协方差矩阵本身有额外的$$ D − N + 1 $$个值为0的特征值。因此我们可以在低维空间中解决特征向量问题，计算代价为$$ O(N^3) $$而不是$$ O(D^3) $$。为了确定特征向量，我们将式（11.28）两侧乘以$$ X^T $$，可得     

$$
\left(\frac{1}{N}X^TX\right)(X^Tv_i) = \lambda_i(X^Tv_i) \tag{12.29}
$$    

从中我们可以看到$$ (X^Tv_i) $$是$$ S $$的一个特征向量，对应的特征值为$$ \lambda_i $$。但是，需要注意，这些特征向量的长度未必等于1。为了确定合适的标准化，我们使用一个常数来对$$ u_i \propto X^Tv_i $$进行重新标度，使得$$ \Vert u_i \Vert = 1 $$。假设$$ v_i $$的长度已经被标准化，那么我们有     

$$
u_i = \frac{1}{(N\lambda_i)^{1/2}}X^Tv_i \tag{12.30}
$$    

总结一下，为了应用这种方法，我们首先计算$$ XX^T $$，然后找到它的特征向量和特征值，之后使用式（12.30）计算原始数据空间的特征向量。
