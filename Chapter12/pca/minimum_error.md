我们现在讨论PCA的另一种形式，基于误差最小化的投影。为了完成这一点，我们引入$$ D $$维基向量的一个完整的单位正交集合$$ \{u_i\} $$，其中$$ i = 1,...,D $$，且满足    

$$
u_i^Tu_j = \delta_{ij} \tag{12.7}
$$    

由于基是完整的，因此每个数据点可以精确地表示为基向量的一个线性组合，即    

$$
x_n = \sum\limits_{i=1}^D\alpha_{ni}u_i \tag{12.8}
$$    

其中，系数$$ \alpha_{ni} $$对于不同的数据点来说是不同的。这对应于将坐标系旋转到了一个由$$ \{u_i\} $$定义的新坐标系，原始的$$ D $$个分量$$ \{x_{n1}|,...,x_{nD}\} $$被替换为一个等价的集合$$ \{\alpha_{n1},...,\alpha_{nD}\} $$。与$$ u_j $$做内积，然后使用单位正交性质，我们有$$ \alpha_{nj} = x_n^Tu_j $$，因此不失一般性，我们有    

$$
x_n = \sum\limits_{i=1}^D(x_n^Tu_i)u_i \tag{12.9}
$$    

然而，我们的目标是使用限定数量$$ M < D $$个变量的一种表示方法来近似数据点，这对应于在低维子空间上的一个投影。不失一般性，$$ M $$维线性子空间可以用前$$ M $$个基向量表示，因此我们可以用下式来近似每个数据点$$ x_n $$    

$$
\tilde{x}_n = \sum\limits_{i=1}^Mz_{ni}u_i + \sum\limits_{i=M+1}^Db_iu_i \tag{12.10}
$$     

其中$$ \{z_{ni}\} $$依赖于特定的数据点，而$$ \{bi\} $$是常数，对于所有数据点都相同。我们可以任意选择$$ \{u_i\}, \{z_{ni}\} $$和$$ \{b_i\} $$，从而最小化由维度降低所引入的失真。作为失真的度量，我们使用原始数据点与它的近似点$$ \tilde{x}_n $$之间的平方距离，在数据集上取平均。因此我们的目标是最小化    

$$
J = \frac{1}{N}\sum\limits_{n=1}^N\Vert x_n - \tilde{x}_n\Vert^2 \tag{12.11}
$$    

首先考虑关于$$ \{z_{ni}\} $$的最小化。消去$$ \tilde{x}_n $$令它关于$$ t_{nj} $$的导数为0，然后使用单位正交的条件，我们有    

$$
z_{nj} = x_n^Tu_j \tag{12.12}
$$    

其中$$ j = 1,...,M $$。类似的，令$$ J $$关于$$ b_i $$的导数等于0，再次使用单位正交的关系得到    

$$
b_j = \bar{x}^Tu_j \tag{12.13}
$$       

其中$$ j = M+1,...,D $$。如果我们消去（12.10）中的$$ z_{ni} $$和$$ b_i $$，使用一般的展开式（12.9），得到    

$$
x_n - \tilde{x}_n = \sum\limits_{i=M+1}^D\{(x_n - \bar{x})^Tu_i\}u_i \tag{12.14}
$$    

从中我们看到，从$$ x_n $$到$$ \tilde{x}_n $$的位移向量位于与主子空间垂直的空间中，因为它是$$ \{u_i\} $$的线性组合，其中$$ i = M+1,...,D $$，如图12.2所示。这与预期相符，因为投影点$$ \tilde{x}_n $$一定位于主子空间内，但是我们可以在那个子空间内自由移动投影点，因此最小的误差由正交投影给出。    

于是，我们得到了失真度量$$ J $$的表达式，它是一个纯粹的关于$$ \{u_i\} $$的函数，形式为     

$$
J = \frac{1}{N}\sum\limits_{n=1}^N\sum\limits_{i=M+1}^D(x_n^Tu_i - \bar{x}^Tu_i)^2 = \sum\limits_{i=M+1}^Du_i^TSu_i \tag{12.15}
$$    

剩下的任务是关于$$ \{u_i\} $$对$$ J $$进行最小化，这必须是具有限制条件的最小化，因为如果不这样，我们会得到$$ u_i = 0 $$这一没有意义的结果。限制来自于单位正交条件，并且正如我们将看到的那样，解可以表示为协方差矩阵的特征向量展开式。在考虑一个形式化的解之前，让我们试着直观地考察一下这个结果。考虑二维数据空间$$ D = 2 $$以及一维主子空间$$ M = 1 $$的情形。我们必须选择一个方向$$ u_2 $$来最小化$$ J = u_2^TSu_2 $$，同时满足限制条件$$ u_2^Tu_2 = 1
$$。使用拉格朗日乘数$$ \lambda_2 $$来强制满足这个限制，我们考虑最小化    

$$
\tilde{J} = u_2^TSu_2 + \lambda_2(1 - u_2^Tu_2) \tag{12.16}
$$    

令关于$$ u_2 $$的导数等于0，我们有$$ Su_2 = \lambda_2u_2 $$，从而$$ u_2 $$是$$ S $$的一个特征向量，且特征值为$$ \lambda_2 $$。因此任何特征向量都会定义失真度量的一个驻点。为了找到$$ J $$在最小值点处的值，我们将$$ u_2 $$的解代回到失真度量中，得到$$ J = \lambda_2 $$。于是，我们通过将$$ u_2 $$选择为对应于两个特征值中较小的那个特征值的特征向量，可以得到$$ J
$$的最小值。因此，我们应该将主子空间与具有较大的特征值的特征向量对齐。这个结果与我们的直觉相符，即为了最小化平均平方投影距离，我们应该将主成分子空间选为穿过数据点的均值并且与最大方差的方向对齐。对于特征值相等的情形，任何主方向的选择都会得到同样的$$ J $$值。    

对于任意的$$ D $$和任意的$$ M < D $$，最小化$$ J $$的一般解都可以通过将$$ \{u_i\} $$选择为协方差矩阵的特征向量的方式得到，即    

$$
Su_i = \lambda_iu_i \tag{12.17}
$$    

其中$$ i = 1,...,D $$，并且与平常一样，特征向量$$ \{u_i\} $$被选为单位正交的。失真度量的对应的值为    

$$
J = \sum\limits_{i=M+1}^D\lambda_i \tag{12.18}
$$    

这就是与主子空间正交的特征值的加和。于是，我们可以通过将这些特征向量选择成$$ D − M $$个最小的特征值对应的特征向量，来得到$$ J $$的最小值，因此定义了主子空间的特征向量是对应于$$ M $$个最大特征值的特征向量。    

虽然我们已经考虑了$$ M < D $$的情形，但是PCA对于$$ M = D $$的情形仍然成立，这种情况下没有维度的降低，仅仅是将坐标轴旋转，与主成分对齐即可。    

最后，值得注意的时，存在一个与此密切相关的线性维度降低的方法，被称为典型相关分析（canonical correlation analysis），或 CCA（Hotelling, 1936; Bach and Jordan, 2002）。PCA操作的对象是一个随机变量，而CCA考虑两个（或更多）的变量，并试图找到具有较高的交叉相关性的线性子空间对，从而在一个子空间中的每个分量都与另一个子空间的一个分量具有相关性。它的解可以表示为一般的特征向量问题。    

