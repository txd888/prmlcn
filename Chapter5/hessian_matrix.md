我们已经说明了如何使用反向传播的方法来得到误差函数关于网络权值的一阶导数。反向传播也可以用来计算误差函数的二阶导数，形式为：

$$
\frac{\partial^2 E}{\partial w_{ji}\partial w_{lk}} \tag{5.78}
$$

。注意，有时把所有的权值和偏置参数看成一个向量$$ w $$的元素$$ w_i $$更方便，此时二阶导数 组成了Hessian矩阵$$ H $$的元素$$ H_{ij} i,j \in {1,...,W} $$，其中$$ W $$是权值和偏置的总数。Hessian矩 阵在神经网络计算的许多方面都有着重要的作用，其中包括：

1. 一些用来训练神经网络的非线性最优化算法是基于误差曲面的由Hessian矩阵控制的二阶性质的(Bishop and Nabney, 2008)。    
2. 对于训练数据的微小改变，Hessian矩阵构成了快速重新训练前馈网络的算法的基础(Bishop, 1991)    
3. Hessian矩阵的逆矩阵用来鉴别神经网络中最不重要的权值，这是网络“剪枝”算法的一部分(LeCun et al., 1990)     
4. Hessian矩阵是贝叶斯神经网络（见5.7节）的拉普拉斯近似的核心。它的逆矩阵可以用来确定训练过的神经网络的预测分布，它的特征值确定了超参数的值，它的行列式可以用来计算模型证据    

神经网络的Hessian矩阵有很多近似计算方法。然而，通过反向传播方法的一个扩展来精确的计算Hessian矩阵。    

对于Hessian矩阵的很多应用来说，一个需要重点考虑的问题是计算效率。如果网络中有$$ W $$个参数（权值和偏置），那么Hessian矩阵的维度为$$ W \times W $$，因此对于数据集里的每个模式来说，计算Hessian矩阵的计算量为$$ O(W^2) $$。正如我们将看到的那样，计算Hessian矩阵的高效方法的计算复杂度确实是$$ O(W^2) $$。    


