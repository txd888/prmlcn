接下来，我们考虑寻找能够使得选定的误差函数$$ E(w) $$达到最小值的权向量$$ w $$。现在，考虑误差函数的几何表示是很有用的。我们可以把误差函数看成位于权空间的一个曲面，如图5.5所示。首先注意到，如果我们在权空间从$$ w $$走到$$ w + \delta w $$，那么误差函数的改变为$$ \delta E \simeq \delta w^T\nabla E(w) $$，其中向量$$ \nabla E(w) $$在误差函数增加速度最大的方向上。由于误差$$ E(w) $$是$$ w
$$的光滑连续函数，因此它的最小值出现在权空间中误差函数梯度等于零的位置上，即

$$
\nabla E(w) = 0 \tag{5.26}
$$

不然我们就可以沿着方向$$ −\nabla E(w) $$走一小步，进一步减小误差。梯度为零的点被称为驻点，它可以进一步细分为极小值点、极大值点和鞍点。    

我们的目标是寻找一个向量$$ w $$使得$$ E(w) $$取最小值。然而，误差函数通常与权值和偏置参数高度非线性的，因此权值空间中会有很多梯度为零（或梯度非常小）的点。实际上，根据5.1.1节的讨论，我们知道，对于任意一个局部极小值点$$ w $$，在权空间中都存在等价的其他极小值点。例如，在图5.1所示的两层神经网络中，有$$ M $$个隐含单元，权空间中的每个点都是$$ M!2^M $$个等价点中的一个。    

此外，通常有多个不等价的驻点，通常会产生多个不等价的极小值。误差函数对于所有的权向量的最小值被称为全局最小值（golobal minimum）。任何其他的使误差函数的值较大的极小值被称为局部极小值（local minima）。对于一个可以成功使用神经网络的应用来说，可能没有必要寻找全局最小值（通常无法知道是否找到了全局最小值），而是通过比较几个局部极小值得到足够好的解。    

由于无法找到方程$$ \nabla E(w) = 0 $$的解析解，因此我们使用迭代的数值方法。连续非线性函数的最优化问题已经被广泛研究，有相当多的文献讨论如何高效地解决它们。大多数方法涉及到为权向量选择某个初始值$$ w_0 $$然后在权空间中进行一系列移动形式为

$$
w^{(\tau + 1)} = w^{(\tau)} + \Delta w^{(tau)} \tag{5.27}
$$

其中$$ tau $$表示迭代的次数。不同的算法涉及到权向量更新$$ \Delta w^{(\tau)} $$的不同选择。许多算法使用梯度信息，因此就需要在每次更新之后计算新的权向量$$ w^{(\tau +1)} $$处的$$ \Delta E(w) $$的值。为了理解梯度信息的重要性，有必要考虑误差函数基于泰勒展开的局部近似。
