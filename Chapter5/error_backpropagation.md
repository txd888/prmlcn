这节我们的目标是找到一种能快速计算出前馈神经网络的误差函数$$ E(w) $$的梯度的技术。我们会看到这可以通过信息在神经网络中交替地向前、向后传播，这种被称为误差反向传播（error backpropagation）有时被简称为“反传”的技术来达到。    

注意，术语“反向传播”在神经网络计算的文献中，用于指代许多不同的事物。如：多层感知器体系节后有时被称为反向传播网络。术语“反向传播”也被用来描述将梯度下降法应用于平方和误差函数的多层感知器的训练过程。为了不让概念发生混淆，仔细研究一下训练过程的本质是很有用的。大部分训练算法涉及到最小化误差函数的一个权值调节的迭代过程。在每一个这样迭代过程中，我们可以分为两个不同的阶段。在第一个阶段，误差函数关于权值的导数必须被计算出来。正如我们稍后会看到的那样，反向传播技术的一个重要的贡献是提供了高效计算这些导数的方法。由于这个阶段误差通过网络进行反向传播，所以我们专门使用反向传播这个术语来描述计算导数的过程。在第二个阶段，导数用于计算权值的调整量。最简单的方法，也是最先由Rumelhart
et
al.(1986)提出的方法，涉及到梯度下降。认识到这两个阶段的不同是很重要的。因此，第一阶段，即为了计算导数而进行的误差在网络中的反向传播阶段，可以应用于许多其他种类的网络，而不仅仅是多层感知器。它也可以应用于其他的误差函数，而不仅仅是简单的平方和误差函数。它也可以用于计算其他类型的导数，如Jacobian矩阵和Hessian矩阵，正如我们将在本章后面看到的那样。类似地，第二阶段，即通过得到的导数调整权值的阶段，可以使用许多本质上比简单的梯度下降更强大的最优化方法处理。     


