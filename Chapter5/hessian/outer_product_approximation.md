当神经网络应用于回归问题时，通常使用形式为

$$
E = \frac{1}{2}\sum\limits_{n=1}^N(y_n - t_n)^2 \tag{5.82}
$$

的平方和误差函数。其中为了记号的简洁，我们考虑单一输出的情形（推广到多个输出也是很直接的）。这样，我们可以把Hessian矩阵写成

$$
H = \nabla\nabla E = \sum\limits_{n=1}^N\nabla y_n\left(\nabla y_n\right)^T + \sum\limits_{n=1}^N(y_n - t_n)\nabla\nabla y_n \tag{5.83}
$$

的形式。如果网络已经在数据集上训练过，且输出$$ y_n $$恰好非常接近$$ t_n $$，那么式（5.83）的第二项会很小，可以被忽略。然而，更一般的情况下，忽略这一项可能更合适。这是因为：回忆一下，根据1.5.5节的讨论，最小化平方和损失的最优函数是目标数据的条件平均。这样，$$ (y_n − t_n) $$是 一个均值为0的随机变量。如果我们假设它的值与式（5.83）右侧的二阶导数项无关，那么在对于$$ n $$的求和项中，整个项的平均值将会等于零。    

通过忽略式（5.83）的第二项，我们就得到了Levenberg-Marquardt近似，或称为外积近似（outer product approximation）（因为此时Hessian矩阵由向量外积的求和构造出来）由

$$
H \simeq \sum\limits_{n=1}^Nb_nb_n^T \tag{5.84}
$$

给出，其中因为输出单元的激活函数是恒等函数，所以$$ b_n = \nabla y_n = \nabla a_n $$。Hessian矩阵外积近似的计算是很容易的，因为它只涉及到误差函数的一阶导数，这可以通过使用标准的反向传播算法在$$ O(W) $$个步骤内高效地求出。通过简单的乘法，矩阵的元素可以在$$ O(W^2) $$个步骤内计算出。需要特别强调的一点是，这种近似只在网络被恰当地训练时才成立，对于一个一般的网络映射，式（5.83）的右侧的二阶导数项通常不能忽略。    

在误差函数为交叉熵误差函数，输出单元激活函数为logistic sigmoid函数的神经网络中，对应的近似为

$$
H \simeq \sum\limits_{n=1}^Ny_n(1-y_n)b_nb_n^T \tag{5.85}
$$

对于具有softmax输出激活函数的多类别网络可以得到类似的结果。    


