我们可以使用外积近似来高效的计算Hessian矩阵的逆（Hassibi and Stork, 1993）。首先，我们把外积近似写成矩阵形式：

$$
H_N = \sum\limits_{n=1}^Nb_nb_n^T \tag{5.86}
$$

其中$$ b_n \equiv \nabla_na_n $$数据点$$ n $$产生的输出单元激活对梯度的贡献。现在，我们推导一个一次使用一个数据点来顺序构造Hessian矩阵的过程。假设，我们已经由开始的$$ L $$个数据点得到Hessian的逆。将第$$ L + 1 $$个数据点的贡献单独写出来，得到

$$
H_{L + 1} = H_L + b_{L+1}b_{L+1}^T \tag{5.87}
$$

为了计算Hessian的逆，现在我们考虑矩阵恒等式：

$$
(M+vv^T)^{-1} = M^{-1} - \frac({M^{-1}v)(v^TM^{-1})}{1 + v^TM^{-1}v} \tag{5.88}
$$

这是一个Woodbury恒等式的一个特例（C.7）。如果我们令$$ H_L = M $$，且$$ b_L + 1 = v $$，得到    

$$
H_{L+1}^{-1} = H_L^{-1} - \frac{H_L^{-1}b_{L+1}b_{L+1}^TH_L^{-1}}{1+b_{L+1}^TH_L^{-1}b_{L+1}} \tag{5.89}
$$

这种方法中，数据点被依次使用，直到$$ L + 1 = N $$整个数据集都被处理。于是，这个结果表示一个通过扫描数据集一次来计算Hessian矩阵的逆矩阵的算法。最开始的矩阵$$ H_0 $$被设置为$$ \alpha I $$，其中$$ \alpha $$是一个较小的量，从而算法实际找的是$$ H + \alpha I $$的逆矩阵。结果对于$$ \alpha $$的精确值不是特别敏感。可以很直接的将这个算法推广到多于一个输出的情形。    

这里，我们注意到，有时Hessian矩阵可以作为网络训练算法的一部分被间接计算出来。特别地，拟牛顿非线性优化算法在训练过程中逐步建立起Hessian矩阵的逆的近似。关于这种算法的详细讨论，可以参考Bishop and Nabney(2008)。
