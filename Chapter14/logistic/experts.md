在14.5.1节，我们考虑了线性回归模型的混合，在14.5.2节，我们讨论了线性分类器的类似的混合。虽然这些简单的混合扩展了线性模型的灵活程度，包含了更复杂的（例如多峰的）预测分布，但是它们仍然具有很大的局限性。我们可以进一步增强这些模型的能力，方法是使得混 合系数本身是输入变量的函数，即    

$$
p(t|x) = \sum\limits_{k=1}^K\pi_k(x)p_k(t|x) \tag{14.53}
$$    

这被称为专家混合（mixture of experts）模型（Jacobs et al.， 1991），其中混合系数$$ \pi_k(x) $$被称为门函数（gating function），各个分量密度$$ p_k(t|x) $$被称为专家（expert）。属于背后的思想是，不同的分量可以对输入空间的不同区域的概率分布进行建模（它们是在它们自己的区域做预测的“专家”），门函数确定哪个分量控制哪个区域。      

门函数$$ \pi_k(x) $$必须满足混合系数通常的限制，即$$ 0 \leq \pi_k(x) \leq 1 $$以及$$ \sum_k\pi_k(x) = 1 $$。因此它们可以通过例如线性softmax函数（4.104）和（4.105）表示。如果专家也是线性（回归或分类）模型，那么整个模型可以使用EM算法高效地调节，在M步骤中要使用迭代重加权最小平方（Jordan and Jacobs， 1994）。    

由于门函数和专家函数使用了线性模型，因此这样的模型仍然有很大的局限性。一个更加灵活的模型时使用多层门函数，得到了专家层次混合（hierarchical mixture of experts）模型或HME模型（Jordan and Jacobs，
1994）。为了理解这个模型的结构，假设一个混合分布，它的每个分量本身都是一个混合分布。对于无条件的混合分布，层次混合简单地等价于一个普通的混合分布。然而，当混合系数与输入相关时，层次模型就变得不普通了。HME模型也可以被看成14.4节讨论的决策树的概率版本，并且与之前一样可以通过最大似然的方式使用EM算法以 及M步骤中的IRLS算法高效计算。Bishop and Svensen (2003)基于变分推断提出了HME的一个贝叶斯方法。    

我们这里不会详细讨论HME。然而，值得指出的一点是，它与5.6节讨论的混合密度网络（mixture density network）有着密切的联系。专家混合的主要的优点在于它可以通过EM算法最优化，其中每个混合分量以及门函数的M步骤涉及到一个凸优化（虽然整体的最优化不是凸优化）。相反，混合密度网络方法的一个优势是分量密度和混合系数共享神经网络的隐含单元。此外，与专家层次混合相比，在混合密度网络中，对输入空间的划分更加放松，因为划分不仅是软划分，并且不限于与坐标轴平行，而且还可以是非线性的。
