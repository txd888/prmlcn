构建一个委员会的最简单的方法是对一组独立的模型的预测取平均。这样的方法的动机可以从频率学家的观点看出来。这种观点考虑偏置和方差之间的折中，它将模型的误差分解为偏置分量和方差分量，其中偏置分量产生于模型和真实的需要预测的函数之间的差异，方差分量表示模型对于单独的数据点的敏感性。回忆一下，根据图3.5，当我们使用正弦数据训练多个多项式函数，然后对得到的函数求平均时，来自方差项的贡献倾向于被抵消掉，从而产生了预测的提升。当我们对一组低偏置的模型（对应于高阶多项式）求平均时，我们得到的对用于生成数 据的正弦函数的精确的预测。    

当然，在实际应用中，我们只有一个单独的数据集，因此我们必须寻找一种方式来表示委员 会中不同模型之间的变化性。一种方法是使用1.2.3节讨论的自助（bootstrap）数据集。考虑一个 回归问题，其中我们试图预测一个连续变量的值，并且假设我们生成了$$ M $$个自助数据集然后使用每个数据集训练处了预测模型的一个独立的副本$$ y_m(x) $$，其中$$ m = 1,...,M $$。委员会预测为    

$$
y_{COM}(x) = \frac{1}{M}\sum\limits_{m=1}^My_m(x) \tag{14.7}
$$    

这个方法被称为自助聚集（bootstrap aggregation）或者打包（bagging）（Breiman， 1996）。    

假设我们试图预测的真实的回归函数为$$ h(x) $$，从而每个模型的输出可以写成真实值加上误差的形式，即    

$$
y_m(x) = h(x) + \epsilon_m(x) \tag{14.8}
$$    

这样，平方和误差函数的形式为    

$$
\mathbb{E}_x[\{y_m(x) - h(x)\}^2] = \mathbb{E}_x[\epsilon_m(x)^2] \tag{14.9}
$$    

其中$$ \mathbb{E}_x[\dot] $$表示关于输入向量$$ x $$的一个频率学的期望。于是，各个模型独立预测的平均误差为    

$$
E_{AV} = \frac{1}{M}\sum\limits_{m=1}^M\mathbb{E}_x[\epsilon_m(x)^2] \tag{14.10}
$$    

类似的，委员会方法的预测（14.7）的期望误差为    

$$
\begin{eqnarray}
E_{COM} &=& \mathbb{E}_x\left[\left\{\frac{1}{M}\sum\limits_{m=1}^My_m(x) - h(x)\right\}^2\right] \\
&=& \mathbb{E}_x\left[\left\{\frac{1}{M}\sum\limits_{m=1}^M\epsilon_m(x)\right\}^2\right] \tag{14.11}
\end{eqnarray}
$$     

如果我们假设误差的均值为0，且不具有相关性，即    

$$
\begin{eqnarray}
\mathbb{E}_x[\epsilon_m(x)] &=& 0 \tag{14.12} \\
\mathbb{E}_x[\epsilon_m(x)\epsilon_l(x)] &=& 0 , m \neq l \tag{14.13}
\end{eqnarray}
$$    

那么我们有    

$$
E_{COM} = \frac{1}{M}E_{AV} \tag{14.14}
$$    

这个显然具有戏剧性的结果表明，一个模型的平均误差可以仅仅通过对模型的$$ M $$个版本求平均的方式减小$$ M $$倍。不幸的是，它依赖于我们的关键假设，即由各个单独的模型产生的误差是不相关的。在实际应用中，误差通常是高度相关的，因此整体的误差下降是通常是很小的。然而，可以证明，委员会误差的期望不会超过各个分量模型的期望误差，即$$ E_{COM} \leq E_{AV} $$。为了得到更显著的提升，我们转向一种更加复杂的构建委员会的方法，被称为提升方法。

