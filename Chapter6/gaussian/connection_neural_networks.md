我们已经看到，神经网络可以表示的函数的范围由隐藏单元的数量$$ M $$控制，并且对于足够大的$$ M $$，一个两层神经网络可以以任意精度近似任意给定的函数。在最大似然的框架中，隐藏单元的数量需要有一定的限制（根据训练集的规模确定限制的程度），来避免过拟现象。然而，从贝叶斯的角度看，根据训练集的规模限制参数的数量几乎毫无意义。     

在贝叶斯神经网络中，参数向量$$ w $$上的先验分布以及网络函数$$ f(x,w) $$产生了函数$$ y(x) $$上的先验概率分布，其中$$ y $$是网络输出向量。Neal(1996)已经证明，在极限$$ M \to \infty $$的情况下，对于$$ w
$$的一大类先验分布，神经网络产生的函数的分布将会趋于高斯过程。然而，应该注意，在这种极限情况下，神经网络的输出变量会变为相互独立的。神经网络的优势之一是输出之间共享隐藏单元，因此它们可以互相“借统计优势”，即与每个隐含结点关联的权值被所有的输出变量影响，而不是只被它们中的某一个影响。这个性质在极限状态下的高斯过程中丢失了。    

我们已经看到，高斯过程由它的协方差（核）函数确定。Williams(1998)给出了在两种具体的隐含单元激活函数（probit和高斯）下，协方差的显式形式。因为以0为中心的高斯权值先验破坏了权空间的平移不变性，所以这些核函数$$ k(x, x') $$是非静止的，即它们不能够表示为差$$ x − x' $$的函数。    

通过直接对协方差函数计算，我们隐式地在权值的分布上进行了积分。如果权值先验由超参数控制，那么它们的值会确定函数的分布的长度标度，这可以通过研究图5.11给出的有限数量单元情形的例子进行理解。注意我们不能解析地对超参数进行积分，而是必须借助6.4节讨论的技术。
