许多回归和分类的线性模型的公式都可以使用核函数自然产生的对偶表示来重写。在我们下一章中讨论支持向量机的时候，这个概念十分重要。这里，我们考虑一个参数通过最小化形式为

$$
J(w) = \frac{1}{2}\sum\limits_{n=1}^N\left\{w^T\phi(x_n) - t_n\right\}^2 + \frac{\lambda}{2}w^Tw \tag{6.2}
$$

正则化的平方和误差函数来确定线性模型。其中$$ \lambda \geq 0 $$。如果我们令$$ J(w) $$关于$$ w $$的梯度等于0，那么我们看到$$ w $$的解是向量$$ \phi(x_n) $$的线性组合的形式，系数是$$ w $$的形式为

$$
w = -\frac{1}{\lambda}\sum\limits_{n=1}^N\{w^T\phi(x_n) - t_n\}\phi(x_n) = \sum\limits_{n=1}^Na_n\phi(x_n) = \Phi^Ta \tag{6.3}
$$

的函数，其中$$ \Phi $$是设计矩阵，第$$ n^{th} $$行由$$ \Phi(x_n)^T $$给出，向量$$ a = (a_1,...,a_N)^T $$，且我们定义了

$$
a_n = -\frac{1}{\lambda}\{w^T\phi(x_n) - t_n\} \tag{6.4}
$$

我们现在不直接对参数向量$$ w $$进行操作，而是使用参数向量$$ a $$重新整理最小平方算法，得到一个对偶表示（dual representation）。如果我们将$$ w = \Phi^Ta $$代入$$ J(w) $$，那么可以得到    

$$
J(a) = \frac{1}{2}a^T\Phi\Phi^T\Phi\Phi^Ta - a^T\Phi\Phi^Tt + \frac{1}{2}t^Tt + \frac{\lambda}{2}a^T\Phi\Phi^Ta \tag{6.5}
$$

其中$$ t = (t_1,...,t_N)^T $$。我们现在定义Gram矩阵$$ K = \Phi\Phi^T $$，它是一个$$ N \times N $$的对称矩阵，元素为    

$$
K_{nm} = \phi(x_n)^T\phi(x_m) = k(x_n,x_m) \tag{6.6}
$$

其中引入了式（6.1）定义的核函数（kernel function）$$ k(x,x') $$。使用Gram矩阵，平方和误差函数可以写成    

$$
J(a) = \frac{1}{2}a^TKKa - a^TKt + \frac{1}{2}t^Tt + \frac{\lambda}a^TKa \tag{6.7}
$$

令$$ J(a) $$关于$$ a $$的梯度为0，得到我们的解：     

$$
a = (K + \lambda I_N)^{-1}t \tag{6.8}
$$

如果我们把它代入线性回归模型中，对于新的输入$$ x $$，我们得到了下面预测    

$$
y(x) = w^T\phi(x) = a^T\Phi\phi(x) = k(x)^T(K + \lambda I_N)^{-1}t \tag{6.9}
$$

其中我们定义了向量$$ k(x) $$，它的元素为$$ k_n(x) = k(x_n,x) $$。因此我们看到对偶公式使得最小平方问题的解完全通过核函数$$ k(x, x') $$表示。这被称为对偶公式，因为$$ a $$的解可以被表示为$$ \phi(x) $$的线性组合，从而我们可以使用参数向量$$ w $$恢复出原始的公式。注意，在$$ x $$处的预测由训练集数据的目标值的线性组合给出。实际上，我们已经在3.3节中得到过这个结果，只不过记号稍微不同。     

在对偶公式中，我们通过对一个$$ N \times N $$的矩阵求逆来确定参数向量$$ a $$，而在原始参数空间公式中，我们要对一个$$ M \times M $$的矩阵求逆来确定$$ w $$。由于$$ N $$通常远大于$$ M $$，因此对偶公式似乎没有实际用处。然而，正如我们将要看到的那样，对偶公式的优点是，它可以完全通过核函数$$ k(x, x') $$来表示。于是，我们可以直接针对核函数进行计算，避免了显式地引入特征向量$$ \phi(x)
$$，这使得我们可以隐式地使用高维特征空间，甚至无限维特征空间。     

基于Gram矩阵的对偶表示的存在是许多线性模型的性质，包括感知器。在6.4节，我们会研究回归的概率线性模型和高斯过程方法的对偶性。当我们在第7章讨论支持向量机的时候，对偶性也起着重要的作用。    


