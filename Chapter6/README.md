在第3、4章中，我们讨论了输入变量$$ x $$通过由可调节参数向量$$ w $$控制的映射$$ y(x,w) $$映射到输出$$ y $$的回归和分类的线性模型。在学习阶段，训练数据被同时用在参数向量的点估计和这个向量上的后验分布确定。然后训练数据就被丢弃，新输入的预测完全基于学习好的参数向量$$ w $$。这样的方法还被运用于像神经网络这样的非线性参数模型。    

然而，有训练数据点或它的一个子集在预测阶段仍然保留并被使用的一类模式识别的技术。如：由每个“核”函数都以训练数据点为中心的线性组合构成的Parzen概率密度模型。类似地，在2.5.2节，我们介绍了一种被称为最近邻的简单的分类方法。这种方法把每个新的测试向量分配为训练数据集里距离最近的样本的标签。这些都是基于存储（memory-based）的方法的例子。基于存储的方法把整个训练数据存储起来，用来对未来的数据点进行预测。通常这种方法需要一个用来定义输入空间任意两个向量之间的相似度的度量。这种方法通常“训练”速度很快，但是对测试数据点的预测速度很慢。    

许多线性参数模型可以被转化为一个等价的预测的基础也是在训练数据点处计算的核函数（kernel function）的线性组合的“对偶表示”。正如我们将看到的那样，对于基于固定非线性特征空间（feature space）映射$$ \phi(x) $$的模型来说，核函数由形式为

$$
k(x,x') = \phi(x)^T\phi(x') \tag{6.1}
$$

的关系给出。根据这个定义，我们看到核函数关于它的参数是对称的，即$$ k(x, x') = k(x', x) $$。核的概念由Aizenman et al.(1964)引入模式识别领域。那篇文章介绍了势函数的方法。之所以被称为势函数，是因为它类似于静电学中的概念。虽然被忽视了很多年，但是Boser et al.(1992)在大边缘分类器的问题中把它重新引入到了机器学习领域。那篇文章提出了支持向量机（support vector machine）的方法。从那时起，这个话题在理论上和实用上都吸引了大家的兴趣。一个最重要的发展是把核方法进行了扩展，使其能处理符号化的物体，从而极大地扩展了这种方法能处理的问题的范围。    

通过考虑式（6.1）中特征空间的恒等映射$$ \phi(x) = x $$，我们就得到了核函数的一个最简单的例子，此时$$ k(x,x') = x^T x' $$，我们把这称为线性核。    

用特征空间的内积的方式表示核的概念使得我们通过核技巧（kernel trick），也被称为核替换（kernel substitution）能够对许多著名的算法进行有趣的扩展。一般的思想是，如果我们有一个算法，它的输入向量$$ x $$只以标量积的形式出现，那么我们可以用一些其他的核来替换这个标量积。例如，核替换方法可以用于主成分分析，从而产生了PCA的非线性变种（Scholkopf et al., 1998）。核替换的其他例子包括最近邻分类器和核Fisher判别函数（Mika et al., 1999; Roth and Steinhage, 2000; Baudat and Anouar, 2000）。    

常用的核函数有各种不同的形式，我们会在本章中遇到若干个核函数的例子。许多核函数只是参数的差值的函数，即$$ k(x,x') = k(x − x') $$，，因为这样核函数对于输入空间的平移具有不变性，所以被称为静止核（stationary kernel）。另一种核函数是同质核（homogeneous kernel），也被称为径向基函数（radial basis function），它只依赖于参数之间的距离（通常是欧几里得距离）的大小，即$$ k(x, x') = k(\Vert x − x' \Vert) $$。    

最近的关于核方法的教材有Scholkopf and Smola(2002),Herbrich(2002)和Shawe-Taylor and Cristianini(2004)。

