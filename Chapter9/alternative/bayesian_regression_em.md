作为说明EM算法应用的第三个例子，我们回到贝叶斯线性回归的证据近似问题。在3.5.2 节，我们通过计算模型证据然后令导数等于零的方式得到了超参数$$ \alpha,\beta $$的值。我们现在使用另一种寻找$$ \alpha,\beta $$的方法，这种方法基于EM算法。回忆一下，我们的目标是关于$$ \alpha,\beta $$最大化由式（3.77）给出的证据函数$$ p(t| \alpha, \beta) $$。由于参数$$ w
$$已经被积分出去，因此我们可以将其当做一个潜在变量，因此我们可以使用EM算法来优化边缘似然函数。在E步骤中，我们计算在给定当前的$$ \alpha,\beta $$的条件下，$$ w $$的后验概率分布，然后使用这个找到完整数据对数似然函数的期望。在M步骤中，我们关于$$ \alpha,\beta $$最大化这个量。我们已经推导出了$$ w $$的后验概率分布，即式（3.49）。这样，完整数据的对数似然函数为     

$$
\ln p(t,w|\alpha,\beta) = \ln p(t|w,\beta) + \ln p(w|\alpha) \tag{9.61}
$$     

其中似然函数$$ p(t|w, \beta) $$和先验概率分布$$ p(w|\alpha) $$分别由式（3.10）和式（3.52）给出。关于$$ w $$的后验概率分布取期望，可得     

$$
\begin{eqnarray}
\mathbb{E}[\ln p(t,w|\alpha,\beta)] = & & \frac{M}{2}\ln\left(\frac{\alpha}{2\pi}\right) - \frac{\alpha}{2}\mathbb{E}[w^Tw] + \frac{N}{2}\ln\left(\frac{\beta}{2\pi}\right) \\
&-& \frac{\beta}{2}\sum\limits_{n=1}^N\mathbb{E}[(t_n - w^t\phi_n)^2] \tag{9.62}
\end{eqnarray}
$$    

令它关于$$ \alpha $$的导数等于0，我们得到了M步骤的重新估计方程     

$$
\alpha = \frac{M}{\mathbb{E}[w^Tw]} = \frac{M}{m_N^Tm_N + Tr(S_N)} \tag{9.63}
$$     

对于$$ beta $$，结果类似。     

注意，这个重新估计方程与直接从证据函数推导出的对应的结果（3.92）的形式稍有不同。然而，两种形式都涉及到了对一个$$ M \times M $$的矩阵进行计算、求逆（或特征分解），因此在每轮迭代时的计算代价是可比的。    

这两种确定$$ \alpha $$的方法显然应该收敛到同样的结果（假设它们找到证据函数的同一个局部极大值）。可以用下面的方法验证。首先注意到$$ \gamma $$的定义为     

$$
\gamma = M - \alpha\sum\limits_{i=1}^M\frac{1}{\lambda_i + \alpha} = M - \alpha Tr(S_N) \tag{9.64}
$$    

在证据函数的驻点处，重估计方程（3.92）一定成立，因此我们可以将$$ \gamma $$替换掉，得到     

$$
\alpha m_N^Tm_N = \gamma = M - \alpha Tr(S_N) \tag{9.65}
$$     

解出$$ \alpha $$，我们得到了式（9.63）的结果，这就是EM的重新估计方程。     

作为最后一个例子，我们考虑一个密切相关的模型，即7.2.1节讨论的用于回归问题的相关向量机。那里，我们直接最大化边缘似然函数来推导超参数$$ \alpha,\beta $$的重估计方程。这里，我们考虑另一种方法，即把权向量$$ w $$看成一个潜在变量，然后使用EM算法。E步骤涉及到寻找权值的后验概率分布，这由公（7.81）给出。在M步骤中，我们最大化完整数据对数似然函数的期望，定义为     

$$
\mathbb{E}[\ln\{p(t|X,w,\beta)p(w|\alpha(\}] \tag{9.66}
$$    

其中期望值是关于使用旧的参数计算的后验概率分布进行计算的。为了计算新的参数值，我们关于$$ \alpha,\beta $$进行最大化，有     

$$
\begin{eqnarray}
\alpha_i^{new} &=& \frac{1}{m_i^2 + \Sigma_{ii}} \tag{9.67} \\
(\beta^{new})^{-1} &=& \frac{\Vert t - \Phi m \Vert^2 + \beta{-1}\sum_i\gamma_i}{N} \tag{9.68}
\end{eqnarray}
$$     

这些重估计方程在形式上等价于直接对边缘似然函数进行最大化得到的重估计方程。
