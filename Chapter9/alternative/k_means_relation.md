对比高斯模型的EM算法与K均值算法，可以看到二者有很强的相似性。K均值算法对数据点的聚类进行了“硬”分配，即每个数据点只属于唯一的聚类，而EM算法基于后验概率分布，进 行了一个“软”分配。实际上，我们可以将K均值算法看成高斯混合模型的EM算法的一个特殊的极限情况，如下所述：    

考虑一个高斯混合模型，其中混合分量的协方差矩阵为$$ \epsilon I $$， $$ \epsilon $$是一个被所有分量共享的方差参数，$$ I $$是单位矩阵，从而    

$$
p(x|\mu_k,\Sigma_k) = \frac{1}{(2\pi\epsilon)^{D/2}}exp\left\{-\frac{1}{2\epsilon}\Vert x - \mu_k\Vert^2\right\} \tag{9.41}
$$    

我们现在考虑$$ K $$个这种形式的高斯分布组成的混合模型的EM算法，其中我们将$$ \epsilon $$看做一个固定的常数，而不是一个需要重新估计的参数。根据式（9.13），对于一个特定的数据点$$ x_n $$，后验概率（或“责任”）为    

$$
\gamma(z_{nk}) = \frac{\pi_kexp\left\{-\frac{\Vert x_n - \mu_k\Vert^2}{2\epsilon}\right\}}{\sum_j\pi_jexp\left\{-\frac{\Vert x_n - \mu_j\Vert^2}{2\epsilon}\right\}} \tag{9.42}
$$

如果我们考虑极限情况$$ \epsilon \to 0 $$，那么我们看到，在分母中，$$ \Vert x_n − \mu_j\Vert^2 $$最小的项将会最慢地趋近于0，因此对于数据点$$ x_n $$，只有项j的“责任”$$ \gamma(z_{nj}) $$趋近于1，其他的项的“责任””$$ \gamma(z_{nj}) $$都趋近于0。因此，在这种极限情况下，我们得到了对数据点聚类的一个硬分配，与K-均值算法相同，从而”$$ \gamma(z_{nj}) \to r_{nk} $$，其中$$ r_{nk}
$$由式（9.2）定义。因此，每个数据点都被分配为距离最近的均值的聚类。    

这样，式（9.17）给出的$$ \mu_k $$的EM重估计就简化为了K均值的结果（9.4）。注意，混合系数（9.22）的重估计公式仅仅将$$ \pi_k $$的值重新设置为等于分配到聚类k中的数据点的比例，虽然这些参数在算法中不再起作用。    

最后，在极限$$ \epsilon \to 0 $$的情况下，式（9.40）给出的完整数据的对数似然函数变成了    

$$
\mathcal{E}_Z[\ln p(X,Z|\mu,\Sigma,\pi)] \to -\frac{1}{2}\sum\limits_{n=1}^N\sum\limits_{k=1}^Kr_{nk}\Vert x_n - \mu_k \Vert^2 + const \tag{9.43}
$$    

因此在极限的情况下，最大化完整数据对数似然函数的期望等价于最小化式（9.1）给出的K-均值算法的失真度量$$ J $$。    

注意，K-均值算法没有估计聚类的协方差，而是只估计了聚类的均值。一个带有一般协方差矩阵的硬分配版本的高斯混合模型被称为椭圆K-均值算法（elliptical K-means algorithm），由Sung and Poggio(1994)提出。
