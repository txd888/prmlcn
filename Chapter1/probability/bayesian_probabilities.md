目前为止，我们通过重复随机事件的发生的频率来考察概率。我们把这叫做经典（classical）或频率论（frequentist）的概率解释。现在我们转向更加通用的贝叶斯（Bayesian）观点，它使频率提供了不确定性的量化描述。    

考虑一个不确定事件，如：月球是否曾经自己的轨道上围绕太阳旋转，或者本世纪末北极冰盖是否会消失。这种事件不能像盒子中的水果一样重复做很多次来定义它们的概率。但是我们通常会想一些办法，如：极地冰块融化的速度。如果我们得到了新的证据，如：从人造卫星获得的新的诊断信息，我们可能会修正关于冰川融化速度的观点。我们对冰川融化速度的评估会影响我们采取的行动，例如在何种程度上的减少温室气体的排放。在这样的情况下，我们定量的描述不确定性，当有少量新的证据是对其进行精确的修正，从而修正接下来所要采取的最优行动或决策。这些都可以优雅的通用的贝叶斯概率观点来实现。    

如果我们想要尊重常识，来做出合理的推断，用概率来表达不确定性不是可选的，而是不可避免的。如：Cox (1946)证明的，如果用数值来表示置信的程度，那么用这种置信度的常识属性的公理集合推导出的一组用来处理置信的程度的规则，等价于概率的加法规则和乘积规则。这首次粗略的证明了概率论可以被当作布尔逻辑的扩展来处理不确定性(Jaynes, 2003)。大量其他的学者也发表了不同的满足这样的不确定性度量的属性的性质集合或者公理集合（Ramsey, 1931; Good, 1950; Savage, 1961; deFinetti, 1970; Lindley,
1982）。在这些情况下，结果的数值完全符合概率的规则。因此把这些看成（贝叶斯观点）概率就很自然了。    

在模式识别领域，对概率有一个更加通用的观点同样是很有帮助的。考虑1.1节中讨论的多项式曲线拟合，以频率论的观点去考察观察到的随机变量$$ t_n $$似乎是合理的。然而我们想强调并量化模型参数$$ w $$的不确定性。我们将会看到，从贝叶斯的观点来说，我们能够使用概率论的机制来描述模型参数$$ w $$或模型选择的不确定性。     

现在，贝叶斯定理有了新的意义。回忆那个盒子中的水果的例子，水果种类的确定，为选择红盒的概率提供了相关的信息。在这个例子中，贝叶斯定理通过观测到的数据提供的证据，把先验概率转化为了后验概率。和将要看到的细节一样，当我们进行数量的推断（如：多项式曲线拟合中的参数$$ w $$），我们可以采用同样的方法。在观测数据之前，我们以先验概率$$ p(w) $$的形式给出了，一些关于参数$$ w $$的假设。观测到的数据$$ D = {t_1,...,t_n}
$$的影响，是通过条件概率$$ p(D|w) $$来表达的，这个在1.2.5节中显示的表达出来。贝叶斯定理的公式：

$$
p(w| D) = \frac{p(D|w)p(w)}{p(D)} \tag{1.43}
$$    

我们可以根据观测到$$ D $$后的后验概率$$ p(w|D) $$来估计$$ w $$的不确定性。    

贝叶斯定理右侧的量$$ p(D|w) $$由观测到的数据集$$ D $$来估计，可以被看成参数向量$$ w $$的似然函数（likelihood function）。不同的参数向量$$ w $$的情况下，观测到的数据集的可能性。注意似然不表示它是$$ w $$的概率分布，它关于$$ w $$的积分也不（一定）等于1。     

给似然函数这个定义之后，我们可以用自然语言来描述贝叶斯定理：    
$$
\text{posterior} \propto \text{likelihood} × \text{prior} \tag{1.44}
$$

其中所有的量都是关于$$ w $$的函数。公式（1.43）中的分母是一个标准化的常数，用来保证左边的后验分布是一个合法的概率密度且积分为1。实际上，对公式（1.43）两边同时对$$ w $$进行积分，我们可以用先验分布和似然函数来表示贝叶斯定理中的分母：    

$$ p(D) = \int p(D|w)P(w)dw \tag{1.45} $$

在贝叶斯和频率论观点中，似然函数$$ p(D|w) $$都起着重要作用。然而，在这两种观点中它的使用方式有着本质的不同。在频率论的观点中，$$ w $$被当作固定的参数，它的值是由某种形式的估计来确定的，这个估计误差是由可能的数据集$$ D $$分布来确定的。与之相比，在贝叶斯观点下，只有一个数据集$$ D $$(即实际观测到的数据集) ，参数的不确定性是通过$$ w $$的概率分布来表示的。    


最大似然（maximum likelihood）是频率论广泛使用的一种估计，其中$$ w $$取使似然函数$$ p(D|w) $$达到最大值的值，也就是使$$ w $$的值等于使观察到的数据集出现的概率最大的值。在机器学习的文献中，似然函数的负对数被称为误差函数（error function）。因为负对数是一个单调递减的函数，最大化似然函数也就是最小化误差。    

自助法（bootstrap）是频率论中一种决定误差的方法(Efron, 1979; Hastie et al., 2001)。这种方法中，使用下面的方式创造多个数据集：假设我们的原始数据集包含$$ N $$个数据点$$ X = {x_1,...,x_N} $$。我们可以通过随机的从$$ X $$中取$$ N $$个数据来创建数据集$$ X_B $$。选取是可以重复的，所以有些$$ X $$中的点可能在$$ X_B $$中出现多次，而有些可能不出现。这样的过程可以重复$$ L $$次，得到$$ L $$个大小为$$ N $$的通过对原数据集$$ X
$$采样得到的数据集。参数估计的统计精确度就可以通过考察不同的自助数据集之间的预测变异性来进行评估。    

贝叶斯观点的一个优点是：很自然的包含了先验知识。例如：假设，掷一枚普通的硬币3次，每次都是正面朝上。经典的最大似然估计硬币正面朝上的概率时，结果会是1，表示将来所有的投掷都会是正面朝上，与之相对的，带有任意合理的先验条件的贝叶斯方法都不会得出这么极端的结论。     

关于频率论和贝叶斯的观点之间的优缺点已经有很多争论，事实上，并没有纯粹的频率论或贝叶斯观点。举个例子，针对贝叶斯方法的一种广泛的批评就是先验概率的选择通常是为了计算的方便而不是为了反映出任何先验的知识。一些观点甚至认为，贝叶斯观点中的结论对先验选择的依赖性是困难的来源。减少对于先验的依赖性是所谓无信息（noninformative）先验的一个研究动机。但是这导致了对比不同模型间的困难，并且当模型选择不好的时候极有可能导致不好的结果。频率论的估计方法在一定的程度上避免了这一问题，并且交叉验证的技术在模型比较等方面也很有用。    

过去几年贝叶斯方法在实际应用中的重要性的逐渐增长，所以本书重点强调贝叶斯观点，在必要的时候讨论一些有用频率学概念。虽然贝叶斯的框架起源于18世纪，但它的实际应用在很长时间内都被执行完整的贝叶斯步骤的困难性所限制，尤其在做预测或比较不同的模型时，需要边缘化（求和或积分）整个参数空间。取样方法的发展，如马尔可夫链（Markov chain）蒙特卡罗（Monte
Carlo）（将在11章中讨论）以及计算机的速度和内存容量的极大提高，打开了在一系列令人映像深刻的问题领域中实际使用贝叶斯方法的大门。其中蒙特卡罗方法非常灵活，可以应用于许多种模型。然而，它在计算上的复杂使得它主要应用于小规模问题。    

最近，许多高效的判别式方法被提出来，例如变种贝叶斯（variational Bayes），期望传播（expectation propagation ）（在第10章中讨论）。它们提供了抽样方法的一种补充替代方法，使得贝叶斯技术能应用在大规模的应用中（Blei et al., 2003）。

