目前为止，我们考虑的是单个目标变量$$ t $$的情况。在一些应用中，我们可能希望预测$$ K > 1 $$的多个目标变量，聚集起来，记作目标向量$$ t $$。这可以通过对于$$ t $$的每个分量，引入一个不同的基函数集合，从而变成了多个独立的回归问题。但是，一个更有趣的且更常用的方法是对目标向量的所有分量使用一组相同的基函数来建模，即

$$
y(x,w) = W^T\phi(x) \tag{3.31}
$$

其中$$ y $$是$$ K $$维列向量，$$ W $$是$$ M \times K $$的参数矩阵，$$ \phi(x) $$是一个元素为$$ \phi_j(x) $$的$$ M $$维列向量，且与之前一样$$ \phi_0(x) = 1 $$。假设我们令目标向量的条件概率分布是一个形式为

$$
p(t|x,W,\beta)=\mathcal{N}(t|W^T\phi(x),\beta^{-1}I) \tag{3.32}
$$

各向同性的高斯分布。如果有观测集合$$ t_1,...,t_N $$，那么可以把它们组合起来得到大小为$$ N \times K $$的矩阵$$ T $$，其中第$$ n^{th} $$行是$$ t_n^T $$。同样的，我们可以组合输入向量$$ x_1,...,x_N $$为矩阵$$ X $$。对数似然函数由

$$
\begin{eqnarray}
\ln p(T|X,W,\beta) &=& \sum\limits_{n=1}^N\ln\mathcal{N}(t_n|W^T\phi(x_n),\beta^{-1}I) \\
&=& \frac{NK}{2}\ln\left(\frac{\beta}{2\pi}\right) - \frac{\beta}{2}\sum\limits_{n=1}^N\Vert t_n-W^T\phi(x_n) \Vert^2 \tag{3.33}
\end{eqnarray}
$$

和之前一样，我们可以对这个函数关于$$ W $$求最大化，得到：    

$$
W_{ML} = (\Phi^T\Phi)^{-1}\Phi^TT \tag{3.34}
$$

如果对每个目标变量$$ t_k $$检验这个结果，得到：    

$$
w_k = (\Phi^T\Phi)^{-1}\Phi^Tt_k = \Phi^+t_k \tag{3.35}
$$

其中$$ t_k $$是分量为$$ t_{nk}, n=1,...,N $$的$$ N $$维列向量。所以得到的回归问题的解是根据不同目标变量分解开的，且只需要计算一次被所有向量$$ w_k $$共享的伪逆矩阵$$ \Phi^+ $$。    

这可以直接的推广到具有任意协方差矩阵的一般的高斯噪声分布。与之前一样，这个问题可以被分解为$$ K $$个独立的回归问题。由于参数$$ W $$只定义了高斯噪声分布的均值，且从2.3.4节中得到多变量高斯的均值的最大似然解独立于协方差，所以这个结果是很显然的。从现在开始，为了简单起见，只考虑单目标变量$$ t $$的情况。
